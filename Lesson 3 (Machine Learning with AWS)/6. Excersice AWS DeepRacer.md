# Exercise: Interpret the reward graph of your first AWS DeepRacer model

## Instructions

Train a model in the  [AWS DeepRacer console](https://console.aws.amazon.com/deepracer/home?region=us-east-1&utm_source=Udacity&utm_medium=Webpage&utm_campaign=Udacity%20AWS%20ML%20Foundations%20Course#getStarted)  and interpret its reward graph.

### Part 1: Train a reinforcement learning model using the AWS DeepRacer console

Practice the knowledge you've learned by training your first reinforcement learning model using the AWS DeepRacer console.

1.  If this is your first time using AWS DeepRacer, choose  **Get started**  from the service landing page, or choose  **Get started with reinforcement learning**  from the main navigation pane.
2.  On the  **Get started with reinforcement learning**  page, under  **Step 2: Create a model and race**, choose  **Create model.**  Alternatively, on the AWS DeepRacer home page, choose **Your models**  from the main navigation pane to open the  **Your models**  page. On the **Your models** page, choose  **Create model**.
3.  On the  **Create model**  page, under  **Environment simulation**, choose a track as a virtual environment to train your AWS DeepRacer agent. Then, choose  **Next.**  For your first run, choose a track with a simple shape and smooth turns. In later iterations, you can choose more complex tracks to progressively improve your models. To train a model for a particular racing event, choose the track most similar to the event track.
4.  On the  **Create model**  page, choose  **Next**.
5.  On the  **Create Model**  page, under  **Race type**, choose a training type. For your first run, choose  **Time trial**. The agent with the default sensor configuration with a single-lens camera is suitable for this type of racing without modifications.
6.  On the  **Create model** page, under  **Training algorithm and hyperparameters**, choose the  **Soft Actor Critic (SAC)**  or  **Proximal Policy Optimization (PPO)** algorithm. In the AWS DeepRacer console, SAC models must be trained in  _continuous action spaces_. PPO models can be trained in either  _continuous_  or  _discrete action spaces_.
7.  On the  **Create model**  page, under  **Training algorithm and hyperparameters**, use the default hyperparameter values as is. Later on, to improve training performance, expand the hyperparameters and experiment with modifying the default hyperparameter values.
8.  On the  **Create model**  page, under  **Agent**, choose **The Original DeepRacer**  or  **The Original DeepRacer (continuous action space)** for your first model. If you use  Soft Actor Critic (SAC) as your training algorithm, we filter your cars so that you can conveniently choose from a selection of compatible continuous action space agents.
9.  On the  **Create model**  page, choose **Next.**
10.  On the  **Create model**  page, under  **Reward function**, use the default reward function example as is for your first model. Later on, you can choose  **Reward function**  examples to select another example function and then choose **Use code**  to accept the selected reward function.
11.  On the  **Create model** page, under  **Stop conditions**, leave the default  **Maximum time**  value as is or set a new value to terminate the training job to help prevent long-running (and possible run-away) training jobs. When experimenting in the early phase of training, you should start with a small value for this parameter and then progressively train for longer amounts of time.
12.  On the  **Create model** page, choose  **Create mode**l to start creating the model and provisioning the training job instance.
13.  After the submission, watch your training job being initialized and then run. The initialization process takes about 6 minutes to change status from  **Initializing**  to  **In progress**.
14.  Watch the  **Reward graph**  and  **Simulation video stream**  to observe the progress of your training job. You can choose the refresh button next to  **Reward graph**  periodically to refresh the  **Reward graph**  until the training job is complete.

**Note:**  The training job is running on the AWS Cloud, so you don't need to keep the AWS DeepRacer console open during training. However, you can come back to the console to check on your model at any point while the job is in progress.

### Part 2: Inspect your reward graph to assess your training progress

As you train and evaluate your first model, you'll want to get a sense of its quality. To do this, inspect your reward graph.

**Find the following on your reward graph:**

-   Average reward
-   Average percentage completion (training)
-   Average percentage completion (evaluation)
-   Best model line
-   Reward primary y-axis
-   Percentage track completion secondary y-axis
-   Iteration x-axis

Review the solution to this exercise for ideas on how to interpret it.

  

![As you train and evaluate your first model, you'll want to get a sense of its quality. To do this, inspect your reward graph. ](https://video.udacity-data.com/topher/2021/April/6083028f_best-model-bar-reward-graph2/best-model-bar-reward-graph2.png)

As you train and evaluate your first model, you'll want to get a sense of its quality. To do this, inspect your reward graph.
# Exercise Solution

To get a sense of how well your training is going, watch the reward graph. Here is a list of its parts and what they do:

-   **Average reward**
    -   This graph represents the average reward the agent earns during a training iteration. The average is calculated by averaging the reward earned across all episodes in the training iteration. An episode begins at the starting line and ends when the agent completes one loop around the track or at the place the vehicle left the track or collided with an object. Toggle the switch to hide this data.
-   **Average percentage completion (training)**
    -   The training graph represents the average percentage of the track completed by the agent in all training episodes in the current training. It shows the performance of the vehicle while experience is being gathered.
-   **Average percentage completion (evaluation)**
    -   While the model is being updated, the performance of the existing model is evaluated. The evaluation graph line is the average percentage of the track completed by the agent in all episodes run during the evaluation period.
-   **Best model line**
    -   This line allows you to see which of your model iterations had the highest average progress during the evaluation. The checkpoint for this iteration will be stored. A checkpoint is a snapshot of a model that is captured after each training (policy-updating) iteration.
-   **Reward primary y-axis**
    -   This shows the reward earned during a training iteration. To read the exact value of a reward, hover your mouse over the data point on the graph.
-   **Percentage track completion secondary y-axis**
    -   This shows you the percentage of the track the agent completed during a training iteration.
-   **Iteration x-axis**
    -   This shows the number of iterations completed during your training job.

  

![Graphic shows elements of a reward graph](https://video.udacity-data.com/topher/2021/April/6082f77f_best-model-bar-reward-graph2/best-model-bar-reward-graph2.png)

List of reward graph parts and what they do

## Reward Graph Interpretation

The following four examples give you a sense of how to interpret the success of your model based on the reward graph. Learning to read these graphs is as much of an art as it is a science and takes time, but reviewing the following four examples will give you a start.

### Needs more training

In the following example, we see there have only been 600 iterations, and the graphs are still going up. We see the evaluation completion percentage has just reached 100%, which is a good sign but isn’t fully consistent yet, and the training completion graph still has a ways to go. This reward function and model are showing promise, but need more training time.

  

![Graph of model that needs more training](https://video.udacity-data.com/topher/2021/April/60826b0a_udacity-reward-graph-needs-more-training/udacity-reward-graph-needs-more-training.png)

Needs more training

### No improvement

In the next example, we can see that the percentage of track completions haven’t gone above around 15 percent and it's been training for quite some time—probably around 6000 iterations or so. This is not a good sign! Consider throwing this model and reward function away and trying a different strategy.

![The reward graph of a model that is not worth keeping.](https://video.udacity-data.com/topher/2021/April/60826871_udacity-reward-graph-bad-graph/udacity-reward-graph-bad-graph.png)

No improvement

### A well-trained model

In the following example graph, we see the evaluation percentage completion reached 100% a while ago, and the training percentage reached 100% roughly 100 or so iterations ago. At this point, the model is well trained. Training it further might lead to the model becoming overfit to this track.

  

### Avoid overfitting

**Overfitting**  or  **overtraining**  is a really important concept in machine learning. With AWS DeepRacer, this can become an issue when a model is trained on a specific track for too long. A good model should be able to make decisions based on the features of the road, such as the sidelines and centerlines, and be able to drive on just about any track.

An  **overtrained**  model, on the other hand, learns to navigate using landmarks specific to an individual track. For example, the agent turns a certain direction when it sees uniquely shaped grass in the background or a specific angle the corner of the wall makes. The resulting model will run beautifully on that specific track, but perform badly on a different virtual track, or even on the same track in a physical environment due to slight variations in angles, textures, and lighting.

  

![This model had been overfit to a specific track.](https://video.udacity-data.com/topher/2021/April/60826bb4_udacity-reward-graph-overfitting/udacity-reward-graph-overfitting.png)

Well-trained - Avoid overfitting

### Adjust hyperparameters

The AWS DeepRacer console's default  **hyperparameters**  are quite effective, but occasionally you may consider adjusting the training hyperparameters. The  **hyperparameters**  are variables that essentially act as settings for the training algorithm that control the performance of your agent during training. We learned, for example, that  the  **learning rate** controls how many new experiences are counted in learning at each step.

In this reward graph example, the training completion graph and the reward graph are swinging high and low. This might suggest an inability to converge, which may be helped by adjusting the learning rate. Imagine if the current weight for a given node is .03, and the optimal weight should be .035, but your learning rate was set to .01. The next training iteration would then swing past optimal to .04, and the following iteration would swing under it to .03 again. If you suspect this, you can reduce the learning rate to .001. A lower learning rate makes learning take longer but can help increase the quality of your model.

![This model's hyperparameters need to be adjusted.](https://video.udacity-data.com/topher/2021/April/60826bfa_udacity-reward-graph-adjust-hyperparameters/udacity-reward-graph-adjust-hyperparameters.png)

Adjust hyperparameters

## Good Job and Good Luck!

Remember: training experience helps both model and reinforcement learning practitioners become a better team. Enter your model in the monthly  [AWS DeepRacer League](https://aws.amazon.com/deepracer/league/?utm_source=Udacity&utm_medium=Webpage&utm_campaign=Udacity%20AWS%20ML%20Foundations%20Course)  races for chances to win prizes and glory while improving your machine learning development skills!
